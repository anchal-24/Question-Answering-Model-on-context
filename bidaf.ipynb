{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZy-0_hrcOF",
        "outputId": "169b7bbb-e2bd-4559-c3ca-9c8e1f25ce0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "nltk.download('punkt')\n",
        "\n",
        "SQUAD_BASE_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "def write_to_file(out_file, line):\n",
        "    out_file.write(line + '\\n')\n",
        "\n",
        "\n",
        "def data_from_json(filename):\n",
        "    with open(filename) as data_file:\n",
        "        data = json.load(data_file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize(sequence, do_lowercase):\n",
        "\n",
        "    if do_lowercase:\n",
        "        tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower()\n",
        "                  for token in nltk.word_tokenize(sequence)]\n",
        "    else:\n",
        "        tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "                  for token in nltk.word_tokenize(sequence)]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def total_examples(dataset):\n",
        "    total = 0\n",
        "    for article in dataset['data']:\n",
        "        for para in article['paragraphs']:\n",
        "            total += len(para['qas'])\n",
        "    return total\n",
        "\n",
        "\n",
        "def maybe_download(base_url, filename, destination_dir, show_progress=True):\n",
        "    local_filename = None\n",
        "  \n",
        "    local_filename, _ = urlretrieve(base_url + filename, filename=os.path.join(destination_dir, filename))\n",
        "\n",
        "         \n",
        "\n",
        "\n",
        "def get_char_word_loc_mapping(context, context_tokens):\n",
        "    acc = ''  # accumulator\n",
        "    current_token_idx = 0  # current word loc\n",
        "    mapping = dict()\n",
        "\n",
        "    # step through original characters\n",
        "    for char_idx, char in enumerate(context):\n",
        "        if char != u' ' and char != u'\\n':  # if it's not a space:\n",
        "            acc += char  # add to accumulator\n",
        "            context_token = context_tokens[current_token_idx]  # current word token\n",
        "            if acc == context_token:  # if the accumulator now matches the current word token\n",
        "                # char loc of the start of this word\n",
        "                syn_start = char_idx - len(acc) + 1\n",
        "                for char_loc in range(syn_start, char_idx + 1):\n",
        "                    mapping[char_loc] = (acc, current_token_idx)  # add to mapping\n",
        "                acc = ''  # reset accumulator\n",
        "                current_token_idx += 1\n",
        "\n",
        "    if current_token_idx != len(context_tokens):\n",
        "        return None\n",
        "    else:\n",
        "        return mapping\n",
        "\n",
        "\n",
        "def preprocess_and_write(dataset, tier, out_dir, squad_version, do_lowercase):\n",
        "    num_exs = 0  # number of examples written to file\n",
        "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
        "    examples = []\n",
        "\n",
        "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
        "        if(articles_id==40):\n",
        "          break\n",
        "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
        "        for pid in range(len(article_paragraphs)):\n",
        "\n",
        "            context = article_paragraphs[pid]['context'].strip()  # string\n",
        "\n",
        "            # The following replacements are suggested in the paper\n",
        "            # BidAF (Seo et al., 2016)\n",
        "            context = context.replace(\"''\", '\" ')\n",
        "            context = context.replace(\"``\", '\" ')\n",
        "\n",
        "            context_tokens = tokenize(context, do_lowercase=do_lowercase) \n",
        "\n",
        "            if do_lowercase:\n",
        "                context = context.lower()\n",
        "\n",
        "            qas = article_paragraphs[pid]['qas']  \n",
        "            charloc2wordloc = get_char_word_loc_mapping(\n",
        "                context, context_tokens)\n",
        "\n",
        "            if charloc2wordloc is None: \n",
        "              #bhaiya yahan pe problem aa rhi hai dekhna\n",
        "                num_mappingprob += len(qas)\n",
        "                continue \n",
        "\n",
        "            for qn in qas:\n",
        "                question = qn['question'].strip() \n",
        "                question_tokens = tokenize(question, do_lowercase=do_lowercase) \n",
        "                ans_text = qn['answers'][0]['text']\n",
        "                ans_start_charloc = qn['answers'][0]['answer_start']\n",
        "                ans_end_charloc = ans_start_charloc + len(ans_text)\n",
        "                if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
        "                    num_spanalignprob += 1\n",
        "                    continue\n",
        "                ans_start_wordloc = charloc2wordloc[ans_start_charloc][1]\n",
        "                ans_end_wordloc = charloc2wordloc[ans_end_charloc - 1][1]\n",
        "                assert ans_start_wordloc <= ans_end_wordloc\n",
        "                ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc + 1]\n",
        "                if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
        "                    num_tokenprob += 1\n",
        "                    continue \n",
        "              \n",
        "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(\n",
        "                    ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
        "\n",
        "                num_exs += 1\n",
        "\n",
        "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
        "    print(\"Processed %i examples of total %i\\n\" %\n",
        "          (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
        "    indices = list(range(len(examples)))\n",
        "    np.random.shuffle(indices)\n",
        "    with open(os.path.join(out_dir, tier + '-v{}.context'.format(squad_version)), 'w', encoding='utf-8') as context_file, \\\n",
        "            open(os.path.join(out_dir, tier + '-v{}.question'.format(squad_version)), 'w', encoding='utf-8') as question_file, \\\n",
        "            open(os.path.join(out_dir, tier + '-v{}.answer'.format(squad_version)), 'w', encoding='utf-8') as ans_text_file, \\\n",
        "            open(os.path.join(out_dir, tier + '-v{}.span'.format(squad_version)), 'w', encoding='utf-8') as span_file:\n",
        "        for i in indices:\n",
        "            (context, question, answer, answer_span) = examples[i]\n",
        "            write_to_file(context_file, context)\n",
        "            write_to_file(question_file, question)\n",
        "            write_to_file(ans_text_file, answer)\n",
        "            write_to_file(span_file, answer_span)\n",
        "\n",
        "def data_download_and_preprocess(squad_version=1.1, do_lowercase=True):\n",
        "    data_dir = os.path.join(base_dir, 'data', 'squad')\n",
        "    print(data_dir)\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    train_filename = \"train-v{}.json\".format(squad_version)\n",
        "    dev_filename = \"dev-v{}.json\".format(squad_version)\n",
        "    maybe_download(SQUAD_BASE_URL, train_filename, data_dir)\n",
        "    train_data = data_from_json(os.path.join(data_dir, train_filename))\n",
        "    print(\"Train data has %i examples total\" % total_examples(train_data))\n",
        "    if not os.path.isfile(os.path.join(data_dir, 'train-v{}.context'.format(squad_version))):\n",
        "        preprocess_and_write(train_data, 'train', data_dir, squad_version, do_lowercase=do_lowercase)\n",
        "    maybe_download(SQUAD_BASE_URL, dev_filename, data_dir)\n",
        "\n",
        "    dev_data = data_from_json(os.path.join(data_dir, dev_filename))\n",
        "    print(\"Dev data has %i examples total\" % total_examples(dev_data))\n",
        "\n",
        "    if not os.path.isfile(os.path.join(data_dir, 'dev-v{}.context'.format(squad_version))):\n",
        "        preprocess_and_write(dev_data, 'dev', data_dir, squad_version, do_lowercase=do_lowercase)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8CkBGRIsk_b",
        "outputId": "ea1d5fb6-acfc-4e68-d2a4-1b42dc2aa4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_download_and_preprocess()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/squad\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing train:   0%|          | 1/442 [00:00<01:04,  6.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train data has 87599 examples total\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing train:   9%|▉         | 39/442 [00:04<00:28, 14.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  6\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  159\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  5948\n",
            "Processed 4976 examples of total 11089\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing dev:   2%|▏         | 1/48 [00:00<00:07,  6.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dev data has 10570 examples total\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Preprocessing dev:  83%|████████▎ | 40/48 [00:03<00:00, 10.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
            "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  114\n",
            "Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems):  4257\n",
            "Processed 4610 examples of total 8981\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8jDd_TtUV9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9VQ2w_esq0P",
        "outputId": "7f923221-0384-4373-ae3e-4de626a9ae00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "from keras.utils import Sequence\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from pymagnitude import Magnitude, MagnitudeUtils\n",
        "\n",
        "\n",
        "class MagnitudeVectors():\n",
        "\n",
        "    def __init__(self, emdim):\n",
        "\n",
        "        base_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
        "\n",
        "        self.fasttext_dim = 300\n",
        "        self.glove_dim = emdim - 300\n",
        "\n",
        "        assert self.glove_dim in [50, 100, 200,\n",
        "                                  300], \"Embedding dimension must be one of the following: 350, 400, 500, 600\"\n",
        "\n",
        "        print(\"Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\")\n",
        "        glove = Magnitude(MagnitudeUtils.download_model('glove/medium/glove.6B.{}d'.format(self.glove_dim),\n",
        "                                                        download_dir=os.path.join(base_dir, 'magnitude')), case_insensitive=True)\n",
        "        fasttext = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M-subword',\n",
        "                                                           download_dir=os.path.join(base_dir, 'magnitude')), case_insensitive=True)\n",
        "        self.vectors = Magnitude(glove, fasttext)\n",
        "\n",
        "    def load_vectors(self):\n",
        "        return self.vectors\n",
        "\n",
        "class BatchGenerator(Sequence):\n",
        "\n",
        "    vectors = None\n",
        "\n",
        "    def __init__(self, gen_type, batch_size, emdim, squad_version, max_passage_length, max_query_length, shuffle):\n",
        "        base_dir = os.path.join(os.getcwd(), 'data')\n",
        "        self.vectors = MagnitudeVectors(emdim).load_vectors()\n",
        "        self.squad_version = squad_version\n",
        "        self.max_passage_length = max_passage_length\n",
        "        self.max_query_length = max_query_length\n",
        "        self.context_file = os.path.join(base_dir, 'squad', gen_type + '-v{}.context'.format(squad_version))\n",
        "        self.question_file = os.path.join(base_dir, 'squad', gen_type + '-v{}.question'.format(squad_version))\n",
        "        self.span_file = os.path.join(base_dir, 'squad', gen_type + '-v{}.span'.format(squad_version))\n",
        "        if self.squad_version == 2.0:\n",
        "            self.is_impossible_file = os.path.join(base_dir, 'squad', gen_type +\n",
        "                                                   '-v{}.is_impossible'.format(squad_version))\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        i = 0\n",
        "        with open(self.span_file, 'r', encoding='utf-8') as f:\n",
        "\n",
        "            for i, _ in enumerate(f):\n",
        "                pass\n",
        "        self.num_of_batches = (i + 1) // self.batch_size\n",
        "        self.indices = np.arange(i + 1)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_of_batches\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_index = (index * self.batch_size) + 1\n",
        "        end_index = ((index + 1) * self.batch_size) + 1\n",
        "        inds = self.indices[start_index:end_index]\n",
        "        contexts = []\n",
        "        with open(self.context_file, 'r', encoding='utf-8') as cf:\n",
        "            for i, line in enumerate(cf, start=1):\n",
        "                line = line[:-1]\n",
        "                if i in inds:\n",
        "                    contexts.append(line.split(' '))\n",
        "        questions = []\n",
        "        with open(self.question_file, 'r', encoding='utf-8') as qf:\n",
        "            for i, line in enumerate(qf, start=1):\n",
        "                line = line[:-1]\n",
        "                if i in inds:\n",
        "                    questions.append(line.split(' '))\n",
        "        answer_spans = []\n",
        "        with open(self.span_file, 'r', encoding='utf-8') as sf:\n",
        "            for i, line in enumerate(sf, start=1):\n",
        "                line = line[:-1]\n",
        "                if i in inds:\n",
        "                    answer_spans.append(line.split(' '))\n",
        "        if self.squad_version == 2.0:\n",
        "            is_impossible = []\n",
        "            with open(self.is_impossible_file, 'r', encoding='utf-8') as isimpf:\n",
        "                for i, line in enumerate(isimpf, start=1):\n",
        "                    line = line[:-1]\n",
        "                    if i in inds:\n",
        "                        is_impossible.append(line)\n",
        "            for i, flag in enumerate(is_impossible):\n",
        "                contexts[i].insert(0, \"unanswerable\")\n",
        "                if flag == \"1\":\n",
        "                    answer_spans[i] = [0, 0]\n",
        "                else:\n",
        "                    answer_spans[i] = [int(val) + 1 for val in answer_spans[i]]\n",
        "\n",
        "        context_batch = self.vectors.query(contexts, pad_to_length=self.max_passage_length)\n",
        "        question_batch = self.vectors.query(questions, pad_to_length=self.max_query_length)\n",
        "        if self.max_passage_length is not None:\n",
        "            span_batch = np.expand_dims(np.array(answer_spans, dtype='float32'), axis=1).clip(0,\n",
        "                                                                                              self.max_passage_length - 1)\n",
        "        else:\n",
        "            span_batch = np.expand_dims(np.array(answer_spans, dtype='float32'), axis=1)\n",
        "        return [context_batch, question_batch], [span_batch]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\n",
        "def load_data_generators(batch_size, emdim, squad_version=1.1, max_passage_length=None, max_query_length=None,\n",
        "                         shuffle=False):\n",
        "    train_generator = BatchGenerator('train', batch_size, emdim, squad_version, max_passage_length, max_query_length,\n",
        "                                     shuffle)\n",
        "    validation_generator = BatchGenerator('dev', batch_size, emdim, squad_version, max_passage_length, max_query_length,\n",
        "                                          shuffle)\n",
        "    return train_generator, validation_generator\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b13a7b6fa8d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMagnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMagnitudeUtils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymagnitude'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYcrgyHTtMkD"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def negative_avg_log_error(y_true, y_pred):\n",
        "\n",
        "    def sum_of_log_probabilities(true_and_pred):\n",
        "        y_true, y_pred_start, y_pred_end = true_and_pred\n",
        "\n",
        "        start_probability = y_pred_start[K.cast(y_true[0], dtype='int32')]\n",
        "        end_probability = y_pred_end[K.cast(y_true[1], dtype='int32')]\n",
        "        return K.log(start_probability) + K.log(end_probability)\n",
        "\n",
        "    y_true = K.squeeze(y_true, axis=1)\n",
        "    y_pred_start = y_pred[:, 0, :]\n",
        "    y_pred_end = y_pred[:, 1, :]\n",
        "    batch_probability_sum = K.map_fn(sum_of_log_probabilities, (y_true, y_pred_start, y_pred_end), dtype='float32')\n",
        "    return -K.mean(batch_probability_sum, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaMUvKzutZhb"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "    def calculate_accuracy(true_and_pred):\n",
        "        y_true, y_pred_start, y_pred_end = true_and_pred\n",
        "\n",
        "        start_probability = y_pred_start[K.cast(y_true[0], dtype='int32')]\n",
        "        end_probability = y_pred_end[K.cast(y_true[1], dtype='int32')]\n",
        "        return (start_probability + end_probability) / 2.0\n",
        "\n",
        "    y_true = K.squeeze(y_true, axis=1)\n",
        "    y_pred_start = y_pred[:, 0, :]\n",
        "    y_pred_end = y_pred[:, 1, :]\n",
        "    accuracy = K.map_fn(calculate_accuracy, (y_true, y_pred_start, y_pred_end), dtype='float32')\n",
        "    return K.mean(accuracy, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abHbVRxotcV9"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
        "from keras.initializers import Constant\n",
        "\n",
        "\n",
        "class Highway(Layer):\n",
        "\n",
        "    activation = None\n",
        "    transform_gate_bias = None\n",
        "\n",
        "    def __init__(self, activation='relu', transform_gate_bias=-1, **kwargs):\n",
        "        self.activation = activation\n",
        "        self.transform_gate_bias = transform_gate_bias\n",
        "        super(Highway, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[-1]\n",
        "        transform_gate_bias_initializer = Constant(self.transform_gate_bias)\n",
        "        input_shape_dense_1 = input_shape[-1]\n",
        "        self.dense_1 = Dense(units=dim, bias_initializer=transform_gate_bias_initializer)\n",
        "        self.dense_1.build(input_shape)\n",
        "        self.dense_2 = Dense(units=dim)\n",
        "        self.dense_2.build(input_shape)\n",
        "        self.trainable_weights = self.dense_1.trainable_weights + self.dense_2.trainable_weights\n",
        "\n",
        "        super(Highway, self).build(input_shape)  \n",
        "    def call(self, x):\n",
        "        dim = K.int_shape(x)[-1]\n",
        "        transform_gate = self.dense_1(x)\n",
        "        transform_gate = Activation(\"sigmoid\")(transform_gate)\n",
        "        carry_gate = Lambda(lambda x: 1.0 - x, output_shape=(dim,))(transform_gate)\n",
        "        transformed_data = self.dense_2(x)\n",
        "        transformed_data = Activation(self.activation)(transformed_data)\n",
        "        transformed_gated = Multiply()([transform_gate, transformed_data])\n",
        "        identity_gated = Multiply()([carry_gate, x])\n",
        "        value = Add()([transformed_gated, identity_gated])\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiDsvvrItql0"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.activations import linear\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class Similarity(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Similarity, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_similarity(self, repeated_context_vectors, repeated_query_vectors):\n",
        "        element_wise_multiply = repeated_context_vectors * repeated_query_vectors\n",
        "        concatenated_tensor = K.concatenate(\n",
        "            [repeated_context_vectors, repeated_query_vectors, element_wise_multiply], axis=-1)\n",
        "        dot_product = K.squeeze(K.dot(concatenated_tensor, self.kernel), axis=-1)\n",
        "        return linear(dot_product + self.bias)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        word_vector_dim = input_shape[0][-1]\n",
        "        weight_vector_dim = word_vector_dim * 3\n",
        "        self.kernel = self.add_weight(name='similarity_weight',\n",
        "                                      shape=(weight_vector_dim, 1),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        self.bias = self.add_weight(name='similarity_bias',\n",
        "                                    shape=(),\n",
        "                                    initializer='ones',\n",
        "                                    trainable=True)\n",
        "        super(Similarity, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context_vectors, query_vectors = inputs\n",
        "        num_context_words = K.shape(context_vectors)[1]\n",
        "        num_query_words = K.shape(query_vectors)[1]\n",
        "        context_dim_repeat = K.concatenate([[1, 1], [num_query_words], [1]], 0)\n",
        "        query_dim_repeat = K.concatenate([[1], [num_context_words], [1, 1]], 0)\n",
        "        repeated_context_vectors = K.tile(K.expand_dims(context_vectors, axis=2), context_dim_repeat)\n",
        "        repeated_query_vectors = K.tile(K.expand_dims(query_vectors, axis=1), query_dim_repeat)\n",
        "        similarity_matrix = self.compute_similarity(repeated_context_vectors, repeated_query_vectors)\n",
        "        return similarity_matrix\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size = input_shape[0][0]\n",
        "        num_context_words = input_shape[0][1]\n",
        "        num_query_words = input_shape[1][1]\n",
        "        return (batch_size, num_context_words, num_query_words)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-b3F95tv8-"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class C2QAttention(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(C2QAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(C2QAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        similarity_matrix, encoded_question = inputs\n",
        "        context_to_query_attention = Softmax(axis=-1)(similarity_matrix)\n",
        "        encoded_question = K.expand_dims(encoded_question, axis=1)\n",
        "        return K.sum(K.expand_dims(context_to_query_attention, axis=-1) * encoded_question, -2)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        similarity_matrix_shape, encoded_question_shape = input_shape\n",
        "        return similarity_matrix_shape[:-1] + encoded_question_shape[-1:]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGLT3S4qtx39"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class Q2CAttention(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Q2CAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Q2CAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        similarity_matrix, encoded_context = inputs\n",
        "        max_similarity = K.max(similarity_matrix, axis=-1)\n",
        "        # by default, axis = -1 in Softmax\n",
        "        context_to_query_attention = Softmax()(max_similarity)\n",
        "        weighted_sum = K.sum(K.expand_dims(context_to_query_attention, axis=-1) * encoded_context, -2)\n",
        "        expanded_weighted_sum = K.expand_dims(weighted_sum, 1)\n",
        "        num_of_repeatations = K.shape(encoded_context)[1]\n",
        "        return K.tile(expanded_weighted_sum, [1, num_of_repeatations, 1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        similarity_matrix_shape, encoded_context_shape = input_shape\n",
        "        return similarity_matrix_shape[:-1] + encoded_context_shape[-1:]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtsnDwQqtziE"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class MergedContext(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MergedContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(MergedContext, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoded_context, context_to_query_attention, query_to_context_attention = inputs\n",
        "        element_wise_multiply1 = encoded_context * context_to_query_attention\n",
        "        element_wise_multiply2 = encoded_context * query_to_context_attention\n",
        "        concatenated_tensor = K.concatenate(\n",
        "            [encoded_context, context_to_query_attention, element_wise_multiply1, element_wise_multiply2], axis=-1)\n",
        "        return concatenated_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        encoded_context_shape, _, _ = input_shape\n",
        "        return encoded_context_shape[:-1] + (encoded_context_shape[-1] * 4, )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upzx7_-Lt40Q"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras.layers import TimeDistributed, Dense\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class SpanBegin(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpanBegin, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        last_dim = input_shape[0][-1] + input_shape[1][-1]\n",
        "        input_shape_dense_1 = input_shape[0][:-1] + (last_dim, )\n",
        "        self.dense_1 = Dense(units=1)\n",
        "        self.dense_1.build(input_shape_dense_1)\n",
        "        self.trainable_weights = self.dense_1.trainable_weights\n",
        "        super(SpanBegin, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        merged_context, modeled_passage = inputs\n",
        "        span_begin_input = K.concatenate([merged_context, modeled_passage])\n",
        "        span_begin_weights = TimeDistributed(self.dense_1)(span_begin_input)\n",
        "        span_begin_probabilities = Softmax()(K.squeeze(span_begin_weights, axis=-1))\n",
        "        return span_begin_probabilities\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        merged_context_shape, _ = input_shape\n",
        "        return merged_context_shape[:-1]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CosrhXbPt9vC"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras.layers import TimeDistributed, Dense, LSTM, Bidirectional\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class SpanEnd(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpanEnd, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        emdim = input_shape[0][-1] // 2\n",
        "        input_shape_bilstm_1 = input_shape[0][:-1] + (emdim*14, )\n",
        "        self.bilstm_1 = Bidirectional(LSTM(emdim, return_sequences=True))\n",
        "        self.bilstm_1.build(input_shape_bilstm_1)\n",
        "        input_shape_dense_1 = input_shape[0][:-1] + (emdim*10, )\n",
        "        self.dense_1 = Dense(units=1)\n",
        "        self.dense_1.build(input_shape_dense_1)\n",
        "        self.trainable_weights = self.bilstm_1.trainable_weights + self.dense_1.trainable_weights\n",
        "        super(SpanEnd, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoded_passage, merged_context, modeled_passage, span_begin_probabilities = inputs\n",
        "        weighted_sum = K.sum(K.expand_dims(span_begin_probabilities, axis=-1) * modeled_passage, -2)\n",
        "        passage_weighted_by_predicted_span = K.expand_dims(weighted_sum, axis=1)\n",
        "        tile_shape = K.concatenate([[1], [K.shape(encoded_passage)[1]], [1]], axis=0)\n",
        "        passage_weighted_by_predicted_span = K.tile(passage_weighted_by_predicted_span, tile_shape)\n",
        "        multiply1 = modeled_passage * passage_weighted_by_predicted_span\n",
        "        span_end_representation = K.concatenate(\n",
        "            [merged_context, modeled_passage, passage_weighted_by_predicted_span, multiply1])\n",
        "\n",
        "        span_end_representation = self.bilstm_1(span_end_representation)\n",
        "\n",
        "        span_end_input = K.concatenate([merged_context, span_end_representation])\n",
        "\n",
        "        span_end_weights = TimeDistributed(self.dense_1)(span_end_input)\n",
        "\n",
        "        span_end_probabilities = Softmax()(K.squeeze(span_end_weights, axis=-1))\n",
        "        return span_end_probabilities\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        _, merged_context_shape, _, _ = input_shape\n",
        "        return merged_context_shape[:-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9myqoht7t_PY"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class CombineOutputs(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CombineOutputs, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(CombineOutputs, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        span_begin_probabilities, span_end_probabilities = inputs\n",
        "        return K.stack([span_begin_probabilities, span_end_probabilities], axis = 1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        number_of_tensors = len(input_shape)\n",
        "        return input_shape[0][0:1] + (number_of_tensors, ) + input_shape[0][1:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwo7Lv9auKNO"
      },
      "source": [
        "def get_best_span(span_begin_probs, span_end_probs, context_length, squad_version, max_span_length):\n",
        "    if len(span_begin_probs.shape) > 2 or len(span_end_probs.shape) > 2:\n",
        "        raise ValueError(\"Input shapes must be (X,) or (1,X)\")\n",
        "    if len(span_begin_probs.shape) == 2:\n",
        "        assert span_begin_probs.shape[0] == 1, \"2D input must have an initial dimension of 1\"\n",
        "        span_begin_probs = span_begin_probs.flatten()\n",
        "    if len(span_end_probs.shape) == 2:\n",
        "        assert span_end_probs.shape[0] == 1, \"2D input must have an initial dimension of 1\"\n",
        "        span_end_probs = span_end_probs.flatten()\n",
        "\n",
        "    max_span_probability = 0\n",
        "    best_word_span = (0, 1)\n",
        "\n",
        "    for i, val1 in enumerate(span_begin_probs):\n",
        "        if squad_version == 2.0 and i == 0:\n",
        "            continue\n",
        "\n",
        "        for j, val2 in enumerate(span_end_probs):\n",
        "            if j > context_length - 1:\n",
        "                break\n",
        "\n",
        "            if (squad_version == 2.0 and j == 0) or (j < i):\n",
        "                continue\n",
        "\n",
        "            if (j - i) >= max_span_length:\n",
        "                break\n",
        "\n",
        "            if val1 * val2 > max_span_probability:\n",
        "                best_word_span = (i, j)\n",
        "                max_span_probability = val1 * val2\n",
        "\n",
        "    if squad_version == 2.0:\n",
        "        if span_begin_probs[0] * span_end_probs[0] > max_span_probability:\n",
        "            best_word_span = (0, 0)\n",
        "            max_span_probability = span_begin_probs[0] * span_end_probs[0]\n",
        "\n",
        "    return best_word_span, max_span_probability\n",
        "\n",
        "\n",
        "def get_word_char_loc_mapping(context, context_tokens):\n",
        "    mapping = {}\n",
        "    idx = 0\n",
        "    for i, word in enumerate(context_tokens):\n",
        "        id = context.find(word, idx)\n",
        "        assert not id == -1, \"Error occurred while mapping word index to character index.. Please report this issue on our GitHub repo.\"\n",
        "\n",
        "        idx = id\n",
        "        mapping[i] = id\n",
        "\n",
        "    assert len(mapping) == len(\n",
        "        context_tokens), \"Error occurred while mapping word index to character index.. Please report this issue on our GitHub repo.\"\n",
        "\n",
        "    return mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jhRGkI7uAj3"
      },
      "source": [
        "from keras.layers import Input, TimeDistributed, LSTM, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "import os\n",
        "\n",
        "\n",
        "class BidirectionalAttentionFlow():\n",
        "\n",
        "    def __init__(self, emdim, max_passage_length=None, max_query_length=None, num_highway_layers=2, num_decoders=1,\n",
        "                 encoder_dropout=0, decoder_dropout=0):\n",
        "        self.emdim = emdim\n",
        "        self.max_passage_length = max_passage_length\n",
        "        self.max_query_length = max_query_length\n",
        "\n",
        "        passage_input = Input(shape=(self.max_passage_length, emdim), dtype='float32', name=\"passage_input\")\n",
        "        question_input = Input(shape=(self.max_query_length, emdim), dtype='float32', name=\"question_input\")\n",
        "\n",
        "        question_embedding = question_input\n",
        "        passage_embedding = passage_input\n",
        "        for i in range(num_highway_layers):\n",
        "            highway_layer = Highway(name='highway_{}'.format(i))\n",
        "            question_layer = TimeDistributed(highway_layer, name=highway_layer.name + \"_qtd\")\n",
        "            question_embedding = question_layer(question_embedding)\n",
        "            passage_layer = TimeDistributed(highway_layer, name=highway_layer.name + \"_ptd\")\n",
        "            passage_embedding = passage_layer(passage_embedding)\n",
        "\n",
        "        encoder_layer = Bidirectional(LSTM(emdim, recurrent_dropout=encoder_dropout,\n",
        "                                           return_sequences=True), name='bidirectional_encoder')\n",
        "        encoded_question = encoder_layer(question_embedding)\n",
        "        encoded_passage = encoder_layer(passage_embedding)\n",
        "\n",
        "        similarity_matrix = Similarity(name='similarity_layer')([encoded_passage, encoded_question])\n",
        "\n",
        "        context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
        "            similarity_matrix, encoded_question])\n",
        "        query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
        "            similarity_matrix, encoded_passage])\n",
        "\n",
        "        merged_context = MergedContext(name='merged_context')(\n",
        "            [encoded_passage, context_to_query_attention, query_to_context_attention])\n",
        "\n",
        "        modeled_passage = merged_context\n",
        "        for i in range(num_decoders):\n",
        "            hidden_layer = Bidirectional(LSTM(emdim, recurrent_dropout=decoder_dropout,\n",
        "                                              return_sequences=True), name='bidirectional_decoder_{}'.format(i))\n",
        "            modeled_passage = hidden_layer(modeled_passage)\n",
        "\n",
        "        span_begin_probabilities = SpanBegin(name='span_begin')([merged_context, modeled_passage])\n",
        "        span_end_probabilities = SpanEnd(name='span_end')(\n",
        "            [encoded_passage, merged_context, modeled_passage, span_begin_probabilities])\n",
        "\n",
        "        output = CombineOutputs(name='combine_outputs')([span_begin_probabilities, span_end_probabilities])\n",
        "\n",
        "        model = Model([passage_input, question_input], [output])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        try:\n",
        "            model = ModelMGPU(model)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        adadelta = Adadelta(lr=0.01)\n",
        "        model.compile(loss=negative_avg_log_error, optimizer=adadelta, metrics=[accuracy])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def load_bidaf(self, path):\n",
        "        custom_objects = {\n",
        "            'Highway': Highway,\n",
        "            'Similarity': Similarity,\n",
        "            'C2QAttention': C2QAttention,\n",
        "            'Q2CAttention': Q2CAttention,\n",
        "            'MergedContext': MergedContext,\n",
        "            'SpanBegin': SpanBegin,\n",
        "            'SpanEnd': SpanEnd,\n",
        "            'CombineOutputs': CombineOutputs,\n",
        "            'negative_avg_log_error': negative_avg_log_error,\n",
        "            'accuracy': accuracy\n",
        "        }\n",
        "\n",
        "        self.model = load_model(path, custom_objects=custom_objects)\n",
        "\n",
        "    def train_model(self, train_generator, steps_per_epoch=None, epochs=1, validation_generator=None,\n",
        "                    validation_steps=None, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0,\n",
        "                    save_history=False, save_model_per_epoch=False):\n",
        "\n",
        "        saved_items_dir = os.path.join(os.path.dirname(__file__), os.pardir, 'saved_items')\n",
        "        if not os.path.exists(saved_items_dir):\n",
        "            os.makedirs(saved_items_dir)\n",
        "\n",
        "        callbacks = []\n",
        "\n",
        "        if save_history:\n",
        "            history_file = os.path.join(saved_items_dir, 'history')\n",
        "            csv_logger = CSVLogger(history_file, append=True)\n",
        "            callbacks.append(csv_logger)\n",
        "\n",
        "        if save_model_per_epoch:\n",
        "            save_model_file = os.path.join(saved_items_dir, 'bidaf_{epoch:02d}.h5')\n",
        "            checkpointer = ModelCheckpoint(filepath=save_model_file, verbose=1)\n",
        "            callbacks.append(checkpointer)\n",
        "\n",
        "        history = self.model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
        "                                           callbacks=callbacks, validation_data=validation_generator,\n",
        "                                           validation_steps=validation_steps, workers=workers,\n",
        "                                           use_multiprocessing=use_multiprocessing, shuffle=shuffle,\n",
        "                                           initial_epoch=initial_epoch)\n",
        "        if not save_model_per_epoch:\n",
        "            self.model.save(os.path.join(saved_items_dir, 'bidaf.h5'))\n",
        "\n",
        "        return history, self.model\n",
        "\n",
        "    def predict_ans(self, passage, question, squad_version=1.1, max_span_length=25, do_lowercase=True,\n",
        "                    return_char_loc=False, return_confidence_score=False):\n",
        "\n",
        "        if type(passage) == list:\n",
        "            assert all(type(pas) == str for pas in passage), \"Input 'passage' must be of type 'string'\"\n",
        "\n",
        "            passage = [pas.strip() for pas in passage]\n",
        "            contexts = []\n",
        "            for pas in passage:\n",
        "                context_tokens = tokenize(pas, do_lowercase)\n",
        "                contexts.append(context_tokens)\n",
        "\n",
        "            if do_lowercase:\n",
        "                original_passage = [pas.lower() for pas in passage]\n",
        "            else:\n",
        "                original_passage = passage\n",
        "\n",
        "        elif type(passage) == str:\n",
        "            passage = passage.strip()\n",
        "            context_tokens = tokenize(passage, do_lowercase)\n",
        "            contexts = [context_tokens, ]\n",
        "\n",
        "            if do_lowercase:\n",
        "                original_passage = [passage.lower(), ]\n",
        "            else:\n",
        "                original_passage = [passage, ]\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Input 'passage' must be either a 'string' or 'list of strings'\")\n",
        "\n",
        "        assert type(passage) == type(\n",
        "            question), \"Both 'passage' and 'question' must be either 'string' or a 'list of strings'\"\n",
        "\n",
        "        if type(question) == list:\n",
        "            assert all(type(ques) == str for ques in question), \"Input 'question' must be of type 'string'\"\n",
        "            assert len(passage) == len(\n",
        "                question), \"Both lists (passage and question) must contain same number of elements\"\n",
        "\n",
        "            questions = []\n",
        "            for ques in question:\n",
        "                question_tokens = tokenize(ques, do_lowercase)\n",
        "                questions.append(question_tokens)\n",
        "\n",
        "        elif type(question) == str:\n",
        "            question_tokens = tokenize(question, do_lowercase)\n",
        "            questions = [question_tokens, ]\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Input 'question' must be either a 'string' or 'list of strings'\")\n",
        "\n",
        "        vectors = MagnitudeVectors(self.emdim).load_vectors()\n",
        "        context_batch = vectors.query(contexts, self.max_passage_length)\n",
        "        question_batch = vectors.query(questions, self.max_query_length)\n",
        "\n",
        "        y = self.model.predict([context_batch, question_batch])\n",
        "        y_pred_start = y[:, 0, :]\n",
        "        y_pred_end = y[:, 1, :]\n",
        "\n",
        "        # clearing the session releases memory by removing the model from memory\n",
        "        # using this, you will need to load model every time before prediction\n",
        "        # K.clear_session()\n",
        "\n",
        "        batch_answer_span = []\n",
        "        batch_confidence_score = []\n",
        "        for sample_id in range(len(contexts)):\n",
        "            answer_span, confidence_score = get_best_span(y_pred_start[sample_id, :], y_pred_end[sample_id, :],\n",
        "                                                          len(contexts[sample_id]), squad_version, max_span_length)\n",
        "            batch_answer_span.append(answer_span)\n",
        "            batch_confidence_score.append(confidence_score)\n",
        "\n",
        "        answers = []\n",
        "        for index, answer_span in enumerate(batch_answer_span):\n",
        "            context_tokens = contexts[index]\n",
        "            start, end = answer_span[0], answer_span[1]\n",
        "\n",
        "            # word index to character index mapping\n",
        "            mapping = get_word_char_loc_mapping(original_passage[index], context_tokens)\n",
        "\n",
        "            char_loc_start = mapping[start]\n",
        "            # [1] => char_loc_end is set to point to one more character after the answer\n",
        "            char_loc_end = mapping[end] + len(context_tokens[end])\n",
        "            # [1] will help us getting a perfect slice without unnecessary increments/decrements\n",
        "            ans = original_passage[index][char_loc_start:char_loc_end]\n",
        "\n",
        "            return_dict = {\n",
        "                \"answer\": ans,\n",
        "            }\n",
        "\n",
        "            if return_char_loc:\n",
        "                return_dict[\"char_loc_start\"] = char_loc_start\n",
        "                return_dict[\"char_loc_end\"] = char_loc_end - 1\n",
        "\n",
        "            if return_confidence_score:\n",
        "                return_dict[\"confidence_score\"] = batch_confidence_score[index]\n",
        "\n",
        "            answers.append(return_dict)\n",
        "\n",
        "        if type(passage) == list:\n",
        "            return answers\n",
        "        else:\n",
        "            return answers[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w2w9cnWumNy"
      },
      "source": [
        "!cp \"/content/drive/My Drive/bidaf_50.h5\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN6wlfTWuMEu",
        "outputId": "7446701e-6c76-4849-972a-ecd1606456ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bidaf_model = BidirectionalAttentionFlow(400)\n",
        "bidaf_model.load_bidaf(\"/content/bidaf_50.h5\")\n",
        "bidaf_model.predict_ans(\"This is a tree\", \"What is this?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "passage_input (InputLayer)      (None, None, 400)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "question_input (InputLayer)     (None, None, 400)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "highway_0_ptd (TimeDistributed) (None, None, 400)    320800      passage_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "highway_0_qtd (TimeDistributed) (None, None, 400)    320800      question_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "highway_1_ptd (TimeDistributed) (None, None, 400)    320800      highway_0_ptd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "highway_1_qtd (TimeDistributed) (None, None, 400)    320800      highway_0_qtd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_encoder (Bidirect (None, None, 800)    2563200     highway_1_qtd[0][0]              \n",
            "                                                                 highway_1_ptd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "similarity_layer (Similarity)   (None, None, None)   2401        bidirectional_encoder[1][0]      \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "context_to_query_attention (C2Q (None, None, 800)    0           similarity_layer[0][0]           \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "query_to_context_attention (Q2C (None, None, 800)    0           similarity_layer[0][0]           \n",
            "                                                                 bidirectional_encoder[1][0]      \n",
            "__________________________________________________________________________________________________\n",
            "merged_context (MergedContext)  (None, None, 3200)   0           bidirectional_encoder[1][0]      \n",
            "                                                                 context_to_query_attention[0][0] \n",
            "                                                                 query_to_context_attention[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_decoder_0 (Bidire (None, None, 800)    11523200    merged_context[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "span_begin (SpanBegin)          (None, None)         4001        merged_context[0][0]             \n",
            "                                                                 bidirectional_decoder_0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "span_end (SpanEnd)              (None, None)         19207201    bidirectional_encoder[1][0]      \n",
            "                                                                 merged_context[0][0]             \n",
            "                                                                 bidirectional_decoder_0[0][0]    \n",
            "                                                                 span_begin[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "combine_outputs (CombineOutputs (None, 2, None)      0           span_begin[0][0]                 \n",
            "                                                                 span_end[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 33,941,603\n",
            "Trainable params: 33,941,603\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'tree'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egI51uPRuT1y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNOfQMpMucPh",
        "outputId": "59f82005-869b-4efc-ef3d-b9e8d1d17ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install flask_ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask_ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (1.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (0.15.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvL7ra9FIxl5",
        "outputId": "f3c7ab0c-825a-4dcf-f184-fb0ec9440cce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,render_template\n",
        "from flask import request\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "@app.route(\"/getAnswer\",methods=['POST','GET'])\n",
        "def getAnswer():\n",
        "    # print(request.form)\n",
        "    context= request.form['context']\n",
        "    q=request.form['question']\n",
        "    ans=bidaf_model.predict_ans(context,q)\n",
        "    return ans['answer']\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
        "<meta name=\"Description\" content=\"Enter your description here\"/>\n",
        "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/css/bootstrap.min.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/all.min.css\">\n",
        "<link rel=\"stylesheet\" href=\"assets/css/style.css\">\n",
        "<title>Adarsh's QA</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <form action=\"/getAnswer\"  method=\"post\">\n",
        "            <div class=\"form-group\">\n",
        "            <label for=\"context\">Context</label>\n",
        "            <input type=\"context\" class=\"form-control\" placeholder=\"Enter context\" name=\"context\" id=\"context\">\n",
        "            </div>\n",
        "            <div class=\"form-group\">\n",
        "            <label for=\"question\">Query:</label>\n",
        "            <input type=\"question\" class=\"form-control\" placeholder=\"Enter question\" name=\"question\" id=\"question\">\n",
        "            </div>\n",
        "        \n",
        "            <button type=\"submit\" class=\"btn btn-primary\">Submit</button>\n",
        "        </form> \n",
        "    </div>\n",
        "  \n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.slim.min.js\"></script>\n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.1/umd/popper.min.js\"></script>\n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/js/bootstrap.min.js\"></script>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://4541957e22d5.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 09:25:14] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Oct/2020 09:25:14] \"\u001b[33mGET /assets/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 09:25:15] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 09:25:30] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Will download magnitude files from the server if they aren't avaialble locally.. So, grab a cup of coffee while the downloading is under progress..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxB2yCl1wfDG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}