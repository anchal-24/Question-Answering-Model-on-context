{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KClp1q7HqPXM",
        "outputId": "b748aa9c-2a13-4b83-8360-3f88189625b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.3MB/s \n",
            "\u001b[?25hCollecting annoy==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/b2/37c2e81952bc2ea7db909b5a698079a432197dc722ac68d61d218878499f/annoy-1.15.2.tar.gz (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 15.8MB/s \n",
            "\u001b[?25hCollecting astor==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
            "Collecting fasteners==0.14.1\n",
            "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting grpcio==1.20.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/a8/8eeb117c46a53768008a7e3892746aa0ea8ea0b669a2a06712eec11ecc33/grpcio-1.20.0-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 21.4MB/s \n",
            "\u001b[?25hCollecting h5py==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 43.5MB/s \n",
            "\u001b[?25hCollecting Keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 48.1MB/s \n",
            "\u001b[?25hCollecting Keras-Applications==1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25hCollecting Keras-Preprocessing==1.0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting lz4==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/fe/66da85ed881031de7cf7de9dd38cc98aec8859824c7bcd3e8a88d255f36d/lz4-2.1.6-cp36-cp36m-manylinux1_x86_64.whl (359kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 47.7MB/s \n",
            "\u001b[?25hCollecting Markdown==3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n",
            "\u001b[?25hCollecting mock==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hCollecting monotonic==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 46.0MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e2/4db8df8f6cddc98e7d7c537245ef2f4e41a1ed17bf0c3177ab3cc6beac7f/numpy-1.16.3-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 238kB/s \n",
            "\u001b[?25hCollecting pbr==5.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 48.9MB/s \n",
            "\u001b[?25hCollecting protobuf==3.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/aa/a858df367b464f5e9452e1c538aa47754d467023850c00b000287750fa77/protobuf-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 47.4MB/s \n",
            "\u001b[?25hCollecting pymagnitude==0.1.120\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/a3/b9a34d22ed8c0ed59b00ff55092129641cdfa09d82f9abdc5088051a5b0c/pymagnitude-0.1.120.tar.gz (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 30.8MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 40.9MB/s \n",
            "\u001b[?25hCollecting scipy==1.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/5f/c48860704092933bf1c4c1574a8de1ffd16bf4fde8bab190d747598844b2/scipy-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8MB 1.5MB/s \n",
            "\u001b[?25hCollecting six==1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting tensorboard==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.2MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 55kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (1.1.0)\n",
            "Collecting torch==0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K     |████████████████████████████████| 519.5MB 32kB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hCollecting Werkzeug==0.15.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/bf/79101bd1d6a2b3fe0888e8d6e039800b173f26b7388308fc4bcc45de8d0a/Werkzeug-0.15.3-py2.py3-none-any.whl (327kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: xxhash==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.3.0)\n",
            "Requirement already satisfied: setuptools>=36 in /usr/local/lib/python3.6/dist-packages (from Markdown==3.1->-r requirements.txt (line 12)) (50.3.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 23)) (0.35.1)\n",
            "Building wheels for collected packages: absl-py, annoy, gast, nltk, pymagnitude, PyYAML\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.7.1-cp36-none-any.whl size=117848 sha256=965e704fdc9e935183e7efaa204381254569b08ebf25ae797ca39f09ad2b4d26\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.15.2-cp36-cp36m-linux_x86_64.whl size=297257 sha256=1b9e7d93eb7845b5d92f17b5ceaf39ef18d71bd65a9c19eca8470bb8ae075d7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/78/3a/1c913a402f4d84f68eb0ea4f7c0de0d392725415c8ebe51068\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=aeb8685f9fa0b4be483d9cf22619656d0a0f1557afb337d4207c599e65549f74\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449909 sha256=9a7ec2682d076203c7073b432e2b8ab905fba5f61ec113da2649e1ad438a9352\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymagnitude: filename=pymagnitude-0.1.120-cp36-cp36m-linux_x86_64.whl size=135918208 sha256=76d96aef3ce51cf15b50b0c3e3a9a1b667fcd9f93bf1854a3f7e8da9a6727207\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/c7/98/cb48b9db35f8d1a7827b764dc36c5515179dc116448a47c8a1\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.1-cp36-cp36m-linux_x86_64.whl size=44075 sha256=02365c4abca250ca3cc11913fa67735aab8b4736e94e1488cd3c9be41eda1331\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "Successfully built absl-py annoy gast nltk pymagnitude PyYAML\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-metadata 0.24.0 has requirement absl-py<0.11,>=0.9, but you'll have absl-py 0.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-hub 0.9.0 has requirement protobuf>=3.8.0, but you'll have protobuf 3.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: nbclient 0.5.1 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, absl-py, annoy, astor, monotonic, fasteners, gast, grpcio, numpy, h5py, scipy, Keras-Preprocessing, Keras-Applications, PyYAML, Keras, lz4, Markdown, pbr, mock, nltk, protobuf, pymagnitude, Werkzeug, tensorboard, tensorflow-estimator, tensorflow, torch, tqdm\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Found existing installation: absl-py 0.10.0\n",
            "    Uninstalling absl-py-0.10.0:\n",
            "      Successfully uninstalled absl-py-0.10.0\n",
            "  Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: grpcio 1.33.1\n",
            "    Uninstalling grpcio-1.33.1:\n",
            "      Successfully uninstalled grpcio-1.33.1\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "  Found existing installation: Markdown 3.3.2\n",
            "    Uninstalling Markdown-3.3.2:\n",
            "      Successfully uninstalled Markdown-3.3.2\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: protobuf 3.12.4\n",
            "    Uninstalling protobuf-3.12.4:\n",
            "      Successfully uninstalled protobuf-3.12.4\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed Keras-2.2.4 Keras-Applications-1.0.7 Keras-Preprocessing-1.0.9 Markdown-3.1 PyYAML-5.1 Werkzeug-0.15.3 absl-py-0.7.1 annoy-1.15.2 astor-0.7.1 fasteners-0.14.1 gast-0.2.2 grpcio-1.20.0 h5py-2.9.0 lz4-2.1.6 mock-2.0.0 monotonic-1.5 nltk-3.4.5 numpy-1.16.3 pbr-5.1.3 protobuf-3.7.1 pymagnitude-0.1.120 scipy-1.2.1 six-1.12.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 torch-0.4.1 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZy-0_hrcOF",
        "outputId": "15590ecb-dcbc-445a-f2c2-3ead6d4aac05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "nltk.download('punkt')\n",
        "\n",
        "SQUAD_BASE_URL = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "def write_to_file(out_file, line):\n",
        "    out_file.write(line + '\\n')\n",
        "\n",
        "\n",
        "def data_from_json(filename):\n",
        "    with open(filename) as data_file:\n",
        "        data = json.load(data_file)\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize(sequence, do_lowercase):\n",
        "\n",
        "    if do_lowercase:\n",
        "        tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower()\n",
        "                  for token in nltk.word_tokenize(sequence)]\n",
        "    else:\n",
        "        tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "                  for token in nltk.word_tokenize(sequence)]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def total_examples(dataset):\n",
        "    total = 0\n",
        "    for article in dataset['data']:\n",
        "        for para in article['paragraphs']:\n",
        "            total += len(para['qas'])\n",
        "    return total\n",
        "\n",
        "\n",
        "def maybe_download(base_url, filename, destination_dir, show_progress=True):\n",
        "    local_filename = None\n",
        "  \n",
        "    local_filename, _ = urlretrieve(base_url + filename, filename=os.path.join(destination_dir, filename))\n",
        "\n",
        "         \n",
        "\n",
        "\n",
        "def get_char_word_loc_mapping(context, context_tokens):\n",
        "    acc = ''  \n",
        "    current_token_idx = 0  \n",
        "    mapping = dict()\n",
        "\n",
        "    for char_idx, char in enumerate(context):\n",
        "        if char != u' ' and char != u'\\n':  \n",
        "            acc += char \n",
        "            context_token = context_tokens[current_token_idx] \n",
        "            if acc == context_token:  \n",
        "                syn_start = char_idx - len(acc) + 1\n",
        "                for char_loc in range(syn_start, char_idx + 1):\n",
        "                    mapping[char_loc] = (acc, current_token_idx)  \n",
        "                acc = ''  \n",
        "                current_token_idx += 1\n",
        "\n",
        "    if current_token_idx != len(context_tokens):\n",
        "        return None\n",
        "    else:\n",
        "        return mapping\n",
        "\n",
        "\n",
        "def preprocess_and_write(dataset, tier, out_dir, do_lowercase):\n",
        "    squad_version=1.1\n",
        "    num_exs = 0  # number of examples written to file\n",
        "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
        "    examples = []\n",
        "\n",
        "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
        "        if(articles_id==40):\n",
        "          break\n",
        "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
        "        for pid in range(len(article_paragraphs)):\n",
        "\n",
        "            context = article_paragraphs[pid]['context'].strip()  # string\n",
        "\n",
        "            # The following replacements are suggested in the paper\n",
        "            # BidAF (Seo et al., 2016)\n",
        "            context = context.replace(\"''\", '\" ')\n",
        "            context = context.replace(\"``\", '\" ')\n",
        "\n",
        "            context_tokens = tokenize(context, do_lowercase=do_lowercase) \n",
        "\n",
        "            if do_lowercase:\n",
        "                context = context.lower()\n",
        "\n",
        "            qas = article_paragraphs[pid]['qas']  \n",
        "            charloc2wordloc = get_char_word_loc_mapping(\n",
        "                context, context_tokens)\n",
        "\n",
        "            if charloc2wordloc is None: \n",
        "              #bhaiya yahan pe problem aa rhi hai dekhna\n",
        "                num_mappingprob += len(qas)\n",
        "                continue \n",
        "\n",
        "            for qn in qas:\n",
        "                question = qn['question'].strip() \n",
        "                question_tokens = tokenize(question, do_lowercase=do_lowercase) \n",
        "                ans_text = qn['answers'][0]['text']\n",
        "                ans_start_charloc = qn['answers'][0]['answer_start']\n",
        "                ans_end_charloc = ans_start_charloc + len(ans_text)\n",
        "                if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
        "                    num_spanalignprob += 1\n",
        "                    continue\n",
        "                ans_start_wordloc = charloc2wordloc[ans_start_charloc][1]\n",
        "                ans_end_wordloc = charloc2wordloc[ans_end_charloc - 1][1]\n",
        "                assert ans_start_wordloc <= ans_end_wordloc\n",
        "                ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc + 1]\n",
        "                if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
        "                    num_tokenprob += 1\n",
        "                    continue \n",
        "              \n",
        "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(\n",
        "                    ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
        "\n",
        "                num_exs += 1\n",
        "\n",
        "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
        "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob)\n",
        "    print(\"Processed %i examples of total %i\\n\" %\n",
        "          (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
        "    indices = list(range(len(examples)))\n",
        "    np.random.shuffle(indices)\n",
        "    with open(os.path.join(out_dir, tier + '-v{}.context'.format(squad_version)), 'w', encoding='utf-8') as context_file, \\\n",
        "            open(os.path.join(out_dir, tier + '-v{}.question'.format(squad_version)), 'w', encoding='utf-8') as question_file, \\\n",
        "            open(os.path.join(out_dir, tier + '-v{}.answer'.format(squad_version)), 'w', encoding='utf-8') as ans_text_file, \\\n",
        "            open(os.path.join(out_dir, tier + '-v{}.span'.format(squad_version)), 'w', encoding='utf-8') as span_file:\n",
        "        for i in indices:\n",
        "            (context, question, answer, answer_span) = examples[i]\n",
        "            write_to_file(context_file, context)\n",
        "            write_to_file(question_file, question)\n",
        "            write_to_file(ans_text_file, answer)\n",
        "            write_to_file(span_file, answer_span)\n",
        "\n",
        "def data_download_and_preprocess( do_lowercase=True):\n",
        "    squad_version=1.1\n",
        "    data_dir = os.path.join(base_dir, 'data', 'squad')\n",
        "    print(data_dir)\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    train_filename = \"train-v{}.json\".format(squad_version)\n",
        "    dev_filename = \"dev-v{}.json\".format(squad_version)\n",
        "    maybe_download(SQUAD_BASE_URL, train_filename, data_dir)\n",
        "    train_data = data_from_json(os.path.join(data_dir, train_filename))\n",
        "    print(\"Train data has %i examples total\" % total_examples(train_data))\n",
        "    if not os.path.isfile(os.path.join(data_dir, 'train-v{}.context'.format(squad_version))):\n",
        "        preprocess_and_write(train_data, 'train', data_dir, do_lowercase=do_lowercase)\n",
        "    maybe_download(SQUAD_BASE_URL, dev_filename, data_dir)\n",
        "\n",
        "    dev_data = data_from_json(os.path.join(data_dir, dev_filename))\n",
        "    print(\"Dev data has %i examples total\" % total_examples(dev_data))\n",
        "\n",
        "    if not os.path.isfile(os.path.join(data_dir, 'dev-v{}.context'.format(squad_version))):\n",
        "        preprocess_and_write(dev_data, 'dev', data_dir, do_lowercase=do_lowercase)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8CkBGRIsk_b",
        "outputId": "e39a8b6e-63cd-44e7-ebcf-d7fbb4bb6729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_download_and_preprocess()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/squad\n",
            "Train data has 87599 examples total\n",
            "Dev data has 10570 examples total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8jDd_TtUV9"
      },
      "source": [
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9VQ2w_esq0P"
      },
      "source": [
        "from keras.utils import Sequence\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "from pymagnitude import Magnitude, MagnitudeUtils\n",
        "\n",
        "\n",
        "class MagnitudeVectors():\n",
        "\n",
        "    def __init__(self, emdim):\n",
        "\n",
        "        base_dir = os.path.join(os.getcwd(), os.pardir, 'data')\n",
        "\n",
        "        self.fasttext_dim = 300\n",
        "        self.glove_dim = emdim - 300\n",
        "\n",
        "        glove = Magnitude(MagnitudeUtils.download_model('glove/medium/glove.6B.{}d'.format(self.glove_dim),\n",
        "                                                        download_dir=os.path.join(base_dir, 'magnitude')), case_insensitive=True)\n",
        "        fasttext = Magnitude(MagnitudeUtils.download_model('fasttext/medium/wiki-news-300d-1M-subword',\n",
        "                                                           download_dir=os.path.join(base_dir, 'magnitude')), case_insensitive=True)\n",
        "        self.vectors = Magnitude(glove, fasttext)\n",
        "\n",
        "    def load_vectors(self):\n",
        "        return self.vectors\n",
        "\n",
        "class BatchGenerator(Sequence):\n",
        "\n",
        "    vectors = None\n",
        "\n",
        "    def __init__(self, gen_type, batch_size, emdim, max_passage_length, max_query_length, shuffle):\n",
        "        squad_version=1.1\n",
        "        base_dir = os.path.join(os.getcwd(), 'data')\n",
        "        self.vectors = MagnitudeVectors(emdim).load_vectors()\n",
        "        self.squad_version = squad_version\n",
        "        self.max_passage_length = max_passage_length\n",
        "        self.max_query_length = max_query_length\n",
        "        self.context_file = os.path.join(base_dir, 'squad', gen_type + '-v{}.context'.format(squad_version))\n",
        "        self.question_file = os.path.join(base_dir, 'squad', gen_type + '-v{}.question'.format(squad_version))\n",
        "        self.span_file = os.path.join(base_dir, 'squad', gen_type + '-v{}.span'.format(squad_version))\n",
        "        if self.squad_version == 2.0:\n",
        "            self.is_impossible_file = os.path.join(base_dir, 'squad', gen_type +\n",
        "                                                   '-v{}.is_impossible'.format(squad_version))\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        i = 0\n",
        "        with open(self.span_file, 'r', encoding='utf-8') as f:\n",
        "\n",
        "            for i, _ in enumerate(f):\n",
        "                pass\n",
        "        self.num_of_batches = (i + 1) // self.batch_size\n",
        "        self.indices = np.arange(i + 1)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_of_batches\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_index = (index * self.batch_size) + 1\n",
        "        end_index = ((index + 1) * self.batch_size) + 1\n",
        "        inds = self.indices[start_index:end_index]\n",
        "        contexts = []\n",
        "        with open(self.context_file, 'r', encoding='utf-8') as cf:\n",
        "            for i, line in enumerate(cf, start=1):\n",
        "                line = line[:-1]\n",
        "                if i in inds:\n",
        "                    contexts.append(line.split(' '))\n",
        "        questions = []\n",
        "        with open(self.question_file, 'r', encoding='utf-8') as qf:\n",
        "            for i, line in enumerate(qf, start=1):\n",
        "                line = line[:-1]\n",
        "                if i in inds:\n",
        "                    questions.append(line.split(' '))\n",
        "        answer_spans = []\n",
        "        with open(self.span_file, 'r', encoding='utf-8') as sf:\n",
        "            for i, line in enumerate(sf, start=1):\n",
        "                line = line[:-1]\n",
        "                if i in inds:\n",
        "                    answer_spans.append(line.split(' '))\n",
        "     \n",
        "\n",
        "        context_batch = self.vectors.query(contexts, pad_to_length=self.max_passage_length)\n",
        "        question_batch = self.vectors.query(questions, pad_to_length=self.max_query_length)\n",
        "        if self.max_passage_length is not None:\n",
        "            span_batch = np.expand_dims(np.array(answer_spans, dtype='float32'), axis=1).clip(0,\n",
        "                                                                                              self.max_passage_length - 1)\n",
        "        else:\n",
        "            span_batch = np.expand_dims(np.array(answer_spans, dtype='float32'), axis=1)\n",
        "        return [context_batch, question_batch], [span_batch]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "\n",
        "def load_data_generators(batch_size, emdim, max_passage_length=None, max_query_length=None,\n",
        "                         shuffle=False):\n",
        "    # squad_version=1.1\n",
        "    train_generator = BatchGenerator('train', batch_size, emdim, max_passage_length, max_query_length,\n",
        "                                     shuffle)\n",
        "    validation_generator = BatchGenerator('dev', batch_size, emdim, max_passage_length, max_query_length,\n",
        "                                          shuffle)\n",
        "    return train_generator, validation_generator\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYcrgyHTtMkD"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def negative_avg_log_error(y_true, y_pred):\n",
        "\n",
        "    def sum_of_log_probabilities(true_and_pred):\n",
        "        y_true, y_pred_start, y_pred_end = true_and_pred\n",
        "\n",
        "        start_probability = y_pred_start[K.cast(y_true[0], dtype='int32')]\n",
        "        end_probability = y_pred_end[K.cast(y_true[1], dtype='int32')]\n",
        "        return K.log(start_probability) + K.log(end_probability)\n",
        "\n",
        "    y_true = K.squeeze(y_true, axis=1)\n",
        "    y_pred_start = y_pred[:, 0, :]\n",
        "    y_pred_end = y_pred[:, 1, :]\n",
        "    batch_probability_sum = K.map_fn(sum_of_log_probabilities, (y_true, y_pred_start, y_pred_end), dtype='float32')\n",
        "    return -K.mean(batch_probability_sum, axis=0)\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaMUvKzutZhb"
      },
      "source": [
        "\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "\n",
        "    def calculate_accuracy(true_and_pred):\n",
        "        y_true, y_pred_start, y_pred_end = true_and_pred\n",
        "\n",
        "        start_probability = y_pred_start[K.cast(y_true[0], dtype='int32')]\n",
        "        end_probability = y_pred_end[K.cast(y_true[1], dtype='int32')]\n",
        "        return (start_probability + end_probability) / 2.0\n",
        "\n",
        "    y_true = K.squeeze(y_true, axis=1)\n",
        "    y_pred_start = y_pred[:, 0, :]\n",
        "    y_pred_end = y_pred[:, 1, :]\n",
        "    accuracy = K.map_fn(calculate_accuracy, (y_true, y_pred_start, y_pred_end), dtype='float32')\n",
        "    return K.mean(accuracy, axis=0)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abHbVRxotcV9"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
        "from keras.initializers import Constant\n",
        "\n",
        "\n",
        "class Highway(Layer):\n",
        "\n",
        "    activation = None\n",
        "    transform_gate_bias = None\n",
        "\n",
        "    def __init__(self, activation='relu', transform_gate_bias=-1, **kwargs):\n",
        "        self.activation = activation\n",
        "        self.transform_gate_bias = transform_gate_bias\n",
        "        super(Highway, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[-1]\n",
        "        transform_gate_bias_initializer = Constant(self.transform_gate_bias)\n",
        "        input_shape_dense_1 = input_shape[-1]\n",
        "        self.dense_1 = Dense(units=dim, bias_initializer=transform_gate_bias_initializer)\n",
        "        self.dense_1.build(input_shape)\n",
        "        self.dense_2 = Dense(units=dim)\n",
        "        self.dense_2.build(input_shape)\n",
        "        self.trainable_weights = self.dense_1.trainable_weights + self.dense_2.trainable_weights\n",
        "\n",
        "        super(Highway, self).build(input_shape)  \n",
        "    def call(self, x):\n",
        "        dim = K.int_shape(x)[-1]\n",
        "        transform_gate = self.dense_1(x)\n",
        "        transform_gate = Activation(\"sigmoid\")(transform_gate)\n",
        "        carry_gate = Lambda(lambda x: 1.0 - x, output_shape=(dim,))(transform_gate)\n",
        "        transformed_data = self.dense_2(x)\n",
        "        transformed_data = Activation(self.activation)(transformed_data)\n",
        "        transformed_gated = Multiply()([transform_gate, transformed_data])\n",
        "        identity_gated = Multiply()([carry_gate, x])\n",
        "        value = Add()([transformed_gated, identity_gated])\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiDsvvrItql0"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.activations import linear\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class Similarity(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Similarity, self).__init__(**kwargs)\n",
        "\n",
        "    def compute_similarity(self, repeated_context_vectors, repeated_query_vectors):\n",
        "        element_wise_multiply = repeated_context_vectors * repeated_query_vectors\n",
        "        concatenated_tensor = K.concatenate(\n",
        "            [repeated_context_vectors, repeated_query_vectors, element_wise_multiply], axis=-1)\n",
        "        dot_product = K.squeeze(K.dot(concatenated_tensor, self.kernel), axis=-1)\n",
        "        return linear(dot_product + self.bias)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        word_vector_dim = input_shape[0][-1]\n",
        "        weight_vector_dim = word_vector_dim * 3\n",
        "        self.kernel = self.add_weight(name='similarity_weight',\n",
        "                                      shape=(weight_vector_dim, 1),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        self.bias = self.add_weight(name='similarity_bias',\n",
        "                                    shape=(),\n",
        "                                    initializer='ones',\n",
        "                                    trainable=True)\n",
        "        super(Similarity, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        context_vectors, query_vectors = inputs\n",
        "        num_context_words = K.shape(context_vectors)[1]\n",
        "        num_query_words = K.shape(query_vectors)[1]\n",
        "        context_dim_repeat = K.concatenate([[1, 1], [num_query_words], [1]], 0)\n",
        "        query_dim_repeat = K.concatenate([[1], [num_context_words], [1, 1]], 0)\n",
        "        repeated_context_vectors = K.tile(K.expand_dims(context_vectors, axis=2), context_dim_repeat)\n",
        "        repeated_query_vectors = K.tile(K.expand_dims(query_vectors, axis=1), query_dim_repeat)\n",
        "        similarity_matrix = self.compute_similarity(repeated_context_vectors, repeated_query_vectors)\n",
        "        return similarity_matrix\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        batch_size = input_shape[0][0]\n",
        "        num_context_words = input_shape[0][1]\n",
        "        num_query_words = input_shape[1][1]\n",
        "        return (batch_size, num_context_words, num_query_words)\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-b3F95tv8-"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class C2QAttention(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(C2QAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(C2QAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        similarity_matrix, encoded_question = inputs\n",
        "        context_to_query_attention = Softmax(axis=-1)(similarity_matrix)\n",
        "        encoded_question = K.expand_dims(encoded_question, axis=1)\n",
        "        return K.sum(K.expand_dims(context_to_query_attention, axis=-1) * encoded_question, -2)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        similarity_matrix_shape, encoded_question_shape = input_shape\n",
        "        return similarity_matrix_shape[:-1] + encoded_question_shape[-1:]\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGLT3S4qtx39"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class Q2CAttention(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Q2CAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Q2CAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        similarity_matrix, encoded_context = inputs\n",
        "        max_similarity = K.max(similarity_matrix, axis=-1)\n",
        "        # by default, axis = -1 in Softmax\n",
        "        context_to_query_attention = Softmax()(max_similarity)\n",
        "        weighted_sum = K.sum(K.expand_dims(context_to_query_attention, axis=-1) * encoded_context, -2)\n",
        "        expanded_weighted_sum = K.expand_dims(weighted_sum, 1)\n",
        "        num_of_repeatations = K.shape(encoded_context)[1]\n",
        "        return K.tile(expanded_weighted_sum, [1, num_of_repeatations, 1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        similarity_matrix_shape, encoded_context_shape = input_shape\n",
        "        return similarity_matrix_shape[:-1] + encoded_context_shape[-1:]\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtsnDwQqtziE"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class MergedContext(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MergedContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(MergedContext, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoded_context, context_to_query_attention, query_to_context_attention = inputs\n",
        "        element_wise_multiply1 = encoded_context * context_to_query_attention\n",
        "        element_wise_multiply2 = encoded_context * query_to_context_attention\n",
        "        concatenated_tensor = K.concatenate(\n",
        "            [encoded_context, context_to_query_attention, element_wise_multiply1, element_wise_multiply2], axis=-1)\n",
        "        return concatenated_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        encoded_context_shape, _, _ = input_shape\n",
        "        return encoded_context_shape[:-1] + (encoded_context_shape[-1] * 4, )\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upzx7_-Lt40Q"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras.layers import TimeDistributed, Dense\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class SpanBegin(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpanBegin, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        last_dim = input_shape[0][-1] + input_shape[1][-1]\n",
        "        input_shape_dense_1 = input_shape[0][:-1] + (last_dim, )\n",
        "        self.dense_1 = Dense(units=1)\n",
        "        self.dense_1.build(input_shape_dense_1)\n",
        "        self.trainable_weights = self.dense_1.trainable_weights\n",
        "        super(SpanBegin, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        merged_context, modeled_passage = inputs\n",
        "        span_begin_input = K.concatenate([merged_context, modeled_passage])\n",
        "        span_begin_weights = TimeDistributed(self.dense_1)(span_begin_input)\n",
        "        span_begin_probabilities = Softmax()(K.squeeze(span_begin_weights, axis=-1))\n",
        "        return span_begin_probabilities\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        merged_context_shape, _ = input_shape\n",
        "        return merged_context_shape[:-1]\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CosrhXbPt9vC"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras.layers.advanced_activations import Softmax\n",
        "from keras.layers import TimeDistributed, Dense, LSTM, Bidirectional\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class SpanEnd(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpanEnd, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        emdim = input_shape[0][-1] // 2\n",
        "        input_shape_bilstm_1 = input_shape[0][:-1] + (emdim*14, )\n",
        "        self.bilstm_1 = Bidirectional(LSTM(emdim, return_sequences=True))\n",
        "        self.bilstm_1.build(input_shape_bilstm_1)\n",
        "        input_shape_dense_1 = input_shape[0][:-1] + (emdim*10, )\n",
        "        self.dense_1 = Dense(units=1)\n",
        "        self.dense_1.build(input_shape_dense_1)\n",
        "        self.trainable_weights = self.bilstm_1.trainable_weights + self.dense_1.trainable_weights\n",
        "        super(SpanEnd, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoded_passage, merged_context, modeled_passage, span_begin_probabilities = inputs\n",
        "        weighted_sum = K.sum(K.expand_dims(span_begin_probabilities, axis=-1) * modeled_passage, -2)\n",
        "        passage_weighted_by_predicted_span = K.expand_dims(weighted_sum, axis=1)\n",
        "        tile_shape = K.concatenate([[1], [K.shape(encoded_passage)[1]], [1]], axis=0)\n",
        "        passage_weighted_by_predicted_span = K.tile(passage_weighted_by_predicted_span, tile_shape)\n",
        "        multiply1 = modeled_passage * passage_weighted_by_predicted_span\n",
        "        span_end_representation = K.concatenate(\n",
        "            [merged_context, modeled_passage, passage_weighted_by_predicted_span, multiply1])\n",
        "\n",
        "        span_end_representation = self.bilstm_1(span_end_representation)\n",
        "\n",
        "        span_end_input = K.concatenate([merged_context, span_end_representation])\n",
        "\n",
        "        span_end_weights = TimeDistributed(self.dense_1)(span_end_input)\n",
        "\n",
        "        span_end_probabilities = Softmax()(K.squeeze(span_end_weights, axis=-1))\n",
        "        return span_end_probabilities\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        _, merged_context_shape, _, _ = input_shape\n",
        "        return merged_context_shape[:-1]\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9myqoht7t_PY"
      },
      "source": [
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "class CombineOutputs(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CombineOutputs, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(CombineOutputs, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        span_begin_probabilities, span_end_probabilities = inputs\n",
        "        return K.stack([span_begin_probabilities, span_end_probabilities], axis = 1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        number_of_tensors = len(input_shape)\n",
        "        return input_shape[0][0:1] + (number_of_tensors, ) + input_shape[0][1:]\n",
        "\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwo7Lv9auKNO"
      },
      "source": [
        "def get_best_span(span_begin_probs, span_end_probs, context_length, max_span_length):\n",
        "    if len(span_begin_probs.shape) > 2 or len(span_end_probs.shape) > 2:\n",
        "        raise ValueError(\"Input shapes must be (X,) or (1,X)\")\n",
        "    if len(span_begin_probs.shape) == 2:\n",
        "        assert span_begin_probs.shape[0] == 1, \"2D input must have an initial dimension of 1\"\n",
        "        span_begin_probs = span_begin_probs.flatten()\n",
        "    if len(span_end_probs.shape) == 2:\n",
        "        assert span_end_probs.shape[0] == 1, \"2D input must have an initial dimension of 1\"\n",
        "        span_end_probs = span_end_probs.flatten()\n",
        "\n",
        "    max_span_probability = 0\n",
        "    best_word_span = (0, 1)\n",
        "\n",
        "    for i, val1 in enumerate(span_begin_probs):\n",
        "        if  i == 0:\n",
        "            continue\n",
        "\n",
        "        for j, val2 in enumerate(span_end_probs):\n",
        "            if j > context_length - 1:\n",
        "                break\n",
        "\n",
        "          \n",
        "            if (j - i) >= max_span_length:\n",
        "                break\n",
        "\n",
        "            if val1 * val2 > max_span_probability:\n",
        "                best_word_span = (i, j)\n",
        "                max_span_probability = val1 * val2\n",
        "\n",
        "   \n",
        "\n",
        "    return best_word_span, max_span_probability\n",
        "\n",
        "\n",
        "def get_word_char_loc_mapping(context, context_tokens):\n",
        "    mapping = {}\n",
        "    idx = 0\n",
        "    for i, word in enumerate(context_tokens):\n",
        "        id = context.find(word, idx)\n",
        "        assert not id == -1, \"Error occurred while mapping word index to character index.. Please report this issue on our GitHub repo.\"\n",
        "\n",
        "        idx = id\n",
        "        mapping[i] = id\n",
        "\n",
        "    assert len(mapping) == len(\n",
        "        context_tokens), \"Error occurred while mapping word index to character index.. Please report this issue on our GitHub repo.\"\n",
        "\n",
        "    return mapping"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jhRGkI7uAj3"
      },
      "source": [
        "from keras.layers import Input, TimeDistributed, LSTM, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "import os\n",
        "\n",
        "\n",
        "class BidirectionalAttentionFlow():\n",
        "\n",
        "    def __init__(self, emdim, max_passage_length=None, max_query_length=None, num_highway_layers=2, num_decoders=1,\n",
        "                 encoder_dropout=0, decoder_dropout=0):\n",
        "        self.emdim = emdim\n",
        "        self.max_passage_length = max_passage_length\n",
        "        self.max_query_length = max_query_length\n",
        "\n",
        "        passage_input = Input(shape=(self.max_passage_length, emdim), dtype='float32', name=\"passage_input\")\n",
        "        question_input = Input(shape=(self.max_query_length, emdim), dtype='float32', name=\"question_input\")\n",
        "\n",
        "        question_embedding = question_input\n",
        "        passage_embedding = passage_input\n",
        "        for i in range(num_highway_layers):\n",
        "            highway_layer = Highway(name='highway_{}'.format(i))\n",
        "            question_layer = TimeDistributed(highway_layer, name=highway_layer.name + \"_qtd\")\n",
        "            question_embedding = question_layer(question_embedding)\n",
        "            passage_layer = TimeDistributed(highway_layer, name=highway_layer.name + \"_ptd\")\n",
        "            passage_embedding = passage_layer(passage_embedding)\n",
        "\n",
        "        encoder_layer = Bidirectional(LSTM(emdim, recurrent_dropout=encoder_dropout,\n",
        "                                           return_sequences=True), name='bidirectional_encoder')\n",
        "        encoded_question = encoder_layer(question_embedding)\n",
        "        encoded_passage = encoder_layer(passage_embedding)\n",
        "\n",
        "        similarity_matrix = Similarity(name='similarity_layer')([encoded_passage, encoded_question])\n",
        "\n",
        "        context_to_query_attention = C2QAttention(name='context_to_query_attention')([\n",
        "            similarity_matrix, encoded_question])\n",
        "        query_to_context_attention = Q2CAttention(name='query_to_context_attention')([\n",
        "            similarity_matrix, encoded_passage])\n",
        "\n",
        "        merged_context = MergedContext(name='merged_context')(\n",
        "            [encoded_passage, context_to_query_attention, query_to_context_attention])\n",
        "\n",
        "        modeled_passage = merged_context\n",
        "        for i in range(num_decoders):\n",
        "            hidden_layer = Bidirectional(LSTM(emdim, recurrent_dropout=decoder_dropout,\n",
        "                                              return_sequences=True), name='bidirectional_decoder_{}'.format(i))\n",
        "            modeled_passage = hidden_layer(modeled_passage)\n",
        "\n",
        "        span_begin_probabilities = SpanBegin(name='span_begin')([merged_context, modeled_passage])\n",
        "        span_end_probabilities = SpanEnd(name='span_end')(\n",
        "            [encoded_passage, merged_context, modeled_passage, span_begin_probabilities])\n",
        "\n",
        "        output = CombineOutputs(name='combine_outputs')([span_begin_probabilities, span_end_probabilities])\n",
        "\n",
        "        model = Model([passage_input, question_input], [output])\n",
        "        model.summary()\n",
        "        adadelta = Adadelta(lr=0.01)\n",
        "        model.compile(loss=negative_avg_log_error, optimizer=adadelta, metrics=[accuracy])\n",
        "        self.model = model\n",
        "\n",
        "    def load_bidaf(self, path):\n",
        "        custom_objects = {\n",
        "            'Highway': Highway,\n",
        "            'Similarity': Similarity,\n",
        "            'C2QAttention': C2QAttention,\n",
        "            'Q2CAttention': Q2CAttention,\n",
        "            'MergedContext': MergedContext,\n",
        "            'SpanBegin': SpanBegin,\n",
        "            'SpanEnd': SpanEnd,\n",
        "            'CombineOutputs': CombineOutputs,\n",
        "            'negative_avg_log_error': negative_avg_log_error,\n",
        "            'accuracy': accuracy\n",
        "        }\n",
        "\n",
        "        self.model = load_model(path, custom_objects=custom_objects)\n",
        "\n",
        "    def train_model(self, train_generator, steps_per_epoch=None, epochs=1, validation_generator=None,\n",
        "                    validation_steps=None, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0,\n",
        "                    save_history=False, save_model_per_epoch=False):\n",
        "\n",
        "        saved_items_dir = os.path.join(os.getcwd(), os.pardir, 'saved_items')\n",
        "        if not os.path.exists(saved_items_dir):\n",
        "            os.makedirs(saved_items_dir)\n",
        "\n",
        "        callbacks = []\n",
        "\n",
        "        if save_history:\n",
        "            history_file = os.path.join(saved_items_dir, 'history')\n",
        "            csv_logger = CSVLogger(history_file, append=True)\n",
        "            callbacks.append(csv_logger)\n",
        "\n",
        "        if save_model_per_epoch:\n",
        "            save_model_file = os.path.join(saved_items_dir, 'bidaf_{epoch:02d}.h5')\n",
        "            checkpointer = ModelCheckpoint(filepath=save_model_file, verbose=1)\n",
        "            callbacks.append(checkpointer)\n",
        "\n",
        "        history = self.model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs,\n",
        "                                           callbacks=callbacks, validation_data=validation_generator,\n",
        "                                           validation_steps=validation_steps, workers=workers,\n",
        "                                           use_multiprocessing=use_multiprocessing, shuffle=shuffle,\n",
        "                                           initial_epoch=initial_epoch)\n",
        "        if not save_model_per_epoch:\n",
        "            self.model.save(os.path.join(saved_items_dir, 'bidaf.h5'))\n",
        "\n",
        "        return history, self.model\n",
        "\n",
        "    def predict_ans(self, passage, question, max_span_length=25, do_lowercase=True,\n",
        "                    return_char_loc=False, return_confidence_score=False):\n",
        "\n",
        "        if type(passage) == list:\n",
        "            assert all(type(pas) == str for pas in passage), \"Input 'passage' must be of type 'string'\"\n",
        "\n",
        "            passage = [pas.strip() for pas in passage]\n",
        "            contexts = []\n",
        "            for pas in passage:\n",
        "                context_tokens = tokenize(pas, do_lowercase)\n",
        "                contexts.append(context_tokens)\n",
        "\n",
        "            if do_lowercase:\n",
        "                original_passage = [pas.lower() for pas in passage]\n",
        "            else:\n",
        "                original_passage = passage\n",
        "\n",
        "        elif type(passage) == str:\n",
        "            passage = passage.strip()\n",
        "            context_tokens = tokenize(passage, do_lowercase)\n",
        "            contexts = [context_tokens, ]\n",
        "\n",
        "            if do_lowercase:\n",
        "                original_passage = [passage.lower(), ]\n",
        "            else:\n",
        "                original_passage = [passage, ]\n",
        "                \n",
        "        if type(question) == list:\n",
        "            questions = []\n",
        "            for ques in question:\n",
        "                question_tokens = tokenize(ques, do_lowercase)\n",
        "                questions.append(question_tokens)\n",
        "        elif type(question) == str:\n",
        "            question_tokens = tokenize(question, do_lowercase)\n",
        "            questions = [question_tokens, ]\n",
        "\n",
        "      \n",
        "\n",
        "        vectors = MagnitudeVectors(self.emdim).load_vectors()\n",
        "        context_batch = vectors.query(contexts, self.max_passage_length)\n",
        "        question_batch = vectors.query(questions, self.max_query_length)\n",
        "\n",
        "        y = self.model.predict([context_batch, question_batch])\n",
        "        y_pred_start = y[:, 0, :]\n",
        "        y_pred_end = y[:, 1, :]\n",
        "\n",
        "     \n",
        "\n",
        "        batch_answer_span = []\n",
        "        batch_confidence_score = []\n",
        "        for sample_id in range(len(contexts)):\n",
        "            answer_span, confidence_score = get_best_span(y_pred_start[sample_id, :], y_pred_end[sample_id, :],\n",
        "                                                          len(contexts[sample_id]), max_span_length)\n",
        "            batch_answer_span.append(answer_span)\n",
        "            batch_confidence_score.append(confidence_score)\n",
        "\n",
        "        answers = []\n",
        "        for index, answer_span in enumerate(batch_answer_span):\n",
        "            context_tokens = contexts[index]\n",
        "            start, end = answer_span[0], answer_span[1]\n",
        "\n",
        "            mapping = get_word_char_loc_mapping(original_passage[index], context_tokens)\n",
        "\n",
        "            char_loc_start = mapping[start]\n",
        "            char_loc_end = mapping[end] + len(context_tokens[end])\n",
        "            ans = original_passage[index][char_loc_start:char_loc_end]\n",
        "\n",
        "            return_dict = {\n",
        "                \"answer\": ans,\n",
        "            }\n",
        "\n",
        "            if return_char_loc:\n",
        "                return_dict[\"char_loc_start\"] = char_loc_start\n",
        "                return_dict[\"char_loc_end\"] = char_loc_end - 1\n",
        "\n",
        "            if return_confidence_score:\n",
        "                return_dict[\"confidence_score\"] = batch_confidence_score[index]\n",
        "\n",
        "            answers.append(return_dict)\n",
        "\n",
        "        if type(passage) == list:\n",
        "            return answers\n",
        "        else:\n",
        "            return answers[0]\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w2w9cnWumNy"
      },
      "source": [
        "!cp \"/content/drive/My Drive/bidaf_50.h5\" ."
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9jjX3uWRh6P",
        "outputId": "20a05d78-df1e-405f-abcf-3f5e5e1d2c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "bidaf_model = BidirectionalAttentionFlow(400)\n",
        "bidaf_model.load_bidaf(\"/content/model2.h5\") # when you want to resume training\n",
        "train_generator, validation_generator = load_data_generators(24, 400)\n",
        "keras_model = bidaf_model.train_model(train_generator, validation_generator=validation_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "passage_input (InputLayer)      (None, None, 400)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "question_input (InputLayer)     (None, None, 400)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "highway_0_ptd (TimeDistributed) (None, None, 400)    320800      passage_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "highway_0_qtd (TimeDistributed) (None, None, 400)    320800      question_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "highway_1_ptd (TimeDistributed) (None, None, 400)    320800      highway_0_ptd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "highway_1_qtd (TimeDistributed) (None, None, 400)    320800      highway_0_qtd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_encoder (Bidirect (None, None, 800)    2563200     highway_1_qtd[0][0]              \n",
            "                                                                 highway_1_ptd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "similarity_layer (Similarity)   (None, None, None)   2401        bidirectional_encoder[1][0]      \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "context_to_query_attention (C2Q (None, None, 800)    0           similarity_layer[0][0]           \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "query_to_context_attention (Q2C (None, None, 800)    0           similarity_layer[0][0]           \n",
            "                                                                 bidirectional_encoder[1][0]      \n",
            "__________________________________________________________________________________________________\n",
            "merged_context (MergedContext)  (None, None, 3200)   0           bidirectional_encoder[1][0]      \n",
            "                                                                 context_to_query_attention[0][0] \n",
            "                                                                 query_to_context_attention[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_decoder_0 (Bidire (None, None, 800)    11523200    merged_context[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "span_begin (SpanBegin)          (None, None)         4001        merged_context[0][0]             \n",
            "                                                                 bidirectional_decoder_0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "span_end (SpanEnd)              (None, None)         19207201    bidirectional_encoder[1][0]      \n",
            "                                                                 merged_context[0][0]             \n",
            "                                                                 bidirectional_decoder_0[0][0]    \n",
            "                                                                 span_begin[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "combine_outputs (CombineOutputs (None, 2, None)      0           span_begin[0][0]                 \n",
            "                                                                 span_end[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 33,941,603\n",
            "Trainable params: 33,941,603\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/1\n",
            "  6/207 [..............................] - ETA: 4:39:47 - loss: 2.6471 - accuracy: 0.4816"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6978c2fd63f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbidaf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_bidaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/model2.h5\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# when you want to resume training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbidaf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-897deb355cc8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_generator, steps_per_epoch, epochs, validation_generator, validation_steps, workers, use_multiprocessing, shuffle, initial_epoch, save_history, save_model_per_epoch)\u001b[0m\n\u001b[1;32m    107\u001b[0m                                            \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                                            \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                                            initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_model_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_items_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bidaf.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN6wlfTWuMEu",
        "outputId": "332256b1-b0d5-4bb2-bf9c-be95ac2e83a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bidaf_model = BidirectionalAttentionFlow(400)\n",
        "bidaf_model.load_bidaf(\"/content/model2.h5\")\n",
        "bidaf_model.predict_ans(\"This is a tree\", \"What is this?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "passage_input (InputLayer)      (None, None, 400)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "question_input (InputLayer)     (None, None, 400)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "highway_0_ptd (TimeDistributed) (None, None, 400)    320800      passage_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "highway_0_qtd (TimeDistributed) (None, None, 400)    320800      question_input[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "highway_1_ptd (TimeDistributed) (None, None, 400)    320800      highway_0_ptd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "highway_1_qtd (TimeDistributed) (None, None, 400)    320800      highway_0_qtd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_encoder (Bidirect (None, None, 800)    2563200     highway_1_qtd[0][0]              \n",
            "                                                                 highway_1_ptd[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "similarity_layer (Similarity)   (None, None, None)   2401        bidirectional_encoder[1][0]      \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "context_to_query_attention (C2Q (None, None, 800)    0           similarity_layer[0][0]           \n",
            "                                                                 bidirectional_encoder[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "query_to_context_attention (Q2C (None, None, 800)    0           similarity_layer[0][0]           \n",
            "                                                                 bidirectional_encoder[1][0]      \n",
            "__________________________________________________________________________________________________\n",
            "merged_context (MergedContext)  (None, None, 3200)   0           bidirectional_encoder[1][0]      \n",
            "                                                                 context_to_query_attention[0][0] \n",
            "                                                                 query_to_context_attention[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_decoder_0 (Bidire (None, None, 800)    11523200    merged_context[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "span_begin (SpanBegin)          (None, None)         4001        merged_context[0][0]             \n",
            "                                                                 bidirectional_decoder_0[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "span_end (SpanEnd)              (None, None)         19207201    bidirectional_encoder[1][0]      \n",
            "                                                                 merged_context[0][0]             \n",
            "                                                                 bidirectional_decoder_0[0][0]    \n",
            "                                                                 span_begin[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "combine_outputs (CombineOutputs (None, 2, None)      0           span_begin[0][0]                 \n",
            "                                                                 span_end[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 33,941,603\n",
            "Trainable params: 33,941,603\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'tree'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNOfQMpMucPh",
        "outputId": "0542fd03-b4f7-4e8a-b153-9594444e110f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install flask_ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask_ngrok) (1.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (0.15.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask_ngrok) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvL7ra9FIxl5",
        "outputId": "5310ae3b-f475-41cb-e398-8bd328ed6b0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,render_template\n",
        "from flask import request\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "@app.route(\"/getAnswer\",methods=['POST','GET'])\n",
        "def getAnswer():\n",
        "    # print(request.form)\n",
        "    context= request.form['context']\n",
        "    q=request.form['question']\n",
        "    print(q,context)\n",
        "    ans=bidaf_model.predict_ans(context,q)\n",
        "    print(ans)\n",
        "    return ans['answer']\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
        "<meta name=\"Description\" content=\"Enter your description here\"/>\n",
        "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/css/bootstrap.min.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/all.min.css\">\n",
        "<link rel=\"stylesheet\" href=\"assets/css/style.css\">\n",
        "<title>Adarsh's QA</title>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <form action=\"/getAnswer\"  method=\"post\">\n",
        "            <div class=\"form-group\">\n",
        "            <label for=\"context\">Context</label>\n",
        "            <input type=\"context\" class=\"form-control\" placeholder=\"Enter context\" name=\"context\" id=\"context\">\n",
        "            </div>\n",
        "            <div class=\"form-group\">\n",
        "            <label for=\"question\">Query:</label>\n",
        "            <input type=\"question\" class=\"form-control\" placeholder=\"Enter question\" name=\"question\" id=\"question\">\n",
        "            </div>\n",
        "        \n",
        "            <button type=\"submit\" class=\"btn btn-primary\">Submit</button>\n",
        "        </form> \n",
        "    </div>\n",
        "  \n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.slim.min.js\"></script>\n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.1/umd/popper.min.js\"></script>\n",
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/js/bootstrap.min.js\"></script>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://437c7957f0df.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:40:49] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:40:50] \"\u001b[33mGET /assets/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:40:50] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:40:59] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What is the capital of India? Delhi is the capital of India\n",
            "{'answer': ''}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:45:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:45:17] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:45:17] \"\u001b[33mGET /assets/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:45:18] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:45:19] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:45:20] \"\u001b[33mGET /assets/css/style.css HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Oct/2020 12:45:24] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Who will submit their project?  Manav, Adarsh and Anchal shall submit their project today\n",
            "{'answer': 'anchal'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:45:33] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What  is a tree This is a tree\n",
            "{'answer': 'tree'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:45:41] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What  is a this? This is a tree\n",
            "{'answer': 'tree'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:45:49] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " What is the capital of India? Delhi is the capital of india\n",
            "{'answer': ''}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:45:52] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What is the capital of India? Delhi is the capital of India\n",
            "{'answer': ''}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:46:06] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Where are we? We are in bhu\n",
            "{'answer': 'bhu'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 12:46:42] \"\u001b[37mPOST /getAnswer HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Which subject is anchal's open elective? Data mining is our anchal's open elective\n",
            "{'answer': 'mining'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Oct/2020 13:11:33] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Oct/2020 13:11:34] \"\u001b[33mGET /assets/css/style.css HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5SVUfYlMYI2",
        "outputId": "05f5ab8e-b396-4f59-b778-1987d094a513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "context=\"Delhi is the capital of India\"\n",
        "q=\"Who is the capital of India?\"\n",
        "bidaf_model.predict_ans(context,q)['answer']\n",
        "# ans=bidaf_model.predict_ans(context,q)\n",
        "# print(ans)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'india'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QurY2x2vENv2"
      },
      "source": [
        "test_data = data_from_json(\"/content/data/squad/dev-v1.1.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxB2yCl1wfDG",
        "outputId": "689f47dd-c18c-4d7d-b771-82d85ddf4c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "c=0\n",
        "t=0\n",
        "last=0\n",
        "for i in range(len(test_data['data'])):\n",
        "  para = test_data['data'][i]['paragraphs']\n",
        "  \n",
        "  for pid in range(len(para)):\n",
        "    context= para[pid]['context']\n",
        "    qas = para[pid]['qas']\n",
        "    for q in qas:\n",
        "      correct = q['answers'][0]['text'].lower().split()\n",
        "      question = q['question']\n",
        "      predicted = bidaf_model.predict_ans(context,question)['answer'].lower().split()\n",
        "      for j in range(len(predicted)):\n",
        "        try:\n",
        "          if(predicted[i].lower()==correct[i]):\n",
        "            c+=1\n",
        "        except:\n",
        "          pass\n",
        "        t+=1\n",
        "    \n",
        "\n",
        "print(c/t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6364231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMcJc5dSEJVe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}